@book{russel,
  author = {S. J. Russell and P. Norvig},
  publisher = {Prentice Hall},
  title = {{Artificial Intelligence: A Modern Approach}},
  year = {2009},
  edition = {3rd}
}

@TECHREPORT{Hei09,
  author = {Heinrich, Gregor},
  title = {Parameter Estimation for Text Analysis},
  institution = {vsonix GmbH and University of Leipzig},
  year = {2009},
  type = {Technical Report Version 2.9},
  abstract = {Presents parameter estimation methods common with discrete probability
	distributions, which is of particular interest in text modeling.
	Starting with maximum likelihood, a posteriori and Bayesian estimation,
	central concepts like conjugate distributions and Bayesian networks
	are reviewed. As an application, the model of latent Dirichlet allocation
	(LDA) is explained in detail with a full derivation of an aaproximate
	inference algorithm based on Gibbs sampling, including a discussion
	of Dirichlet hyperparameter estimation.},
}

@TECHREPORT{RH10,
  author = {Resnik, Philip and Hardisty, Eric},
  title = {Gibbs Sampling for the Uninitiated},
  institution = {University of Maryland},
  year = {2010},
  type = {Technical Report CS-TR-4956, UMIACS-TR-2010-04, LAMP-153},
  abstract = {This document is intended for computer scientists who would like to
	try out a Markov Chain Monte Carlo (MCMC) technique, particularly
	in order to do inference with Bayesian models on problems related
	to text processing. We try to keep theory to the absolute minimum
	needed, though we work through the details much more explicitly than
	you usually see even in "introductory" explanations. That means we've
	attempted to be ridiculously explicit in our exposition and notation.
	
	After providing the reasons and reasoning behind Gibbs sampling (and
	at least nodding our heads in the direction of theory), we work through
	an example application in detail---the derivation of a Gibbs sampler
	for a Na\"{i}ve Bayes model. Along with the example, we discuss some
	practical implementation issues, including the integrating out of
	continuous parameters when possible. We conclude with some pointers
	to literature that we've found to be somewhat more friendly to uninitiated
	readers. 
	
	Note: as of June 3, 2010 we have corrected some small errors in the
	original April 2010 report.},
  keywords = {Gibbs Sampling; Markov Chain Monte Carlo; Na\"{i}ve Bayes; Bayesian
	Inference; Tutorial},
  url = {http://drum.lib.umd.edu/bitstream/1903/10058/3/gsfu.pdf}
}

@ELECTRONIC{Kni09,
  author = {Knight, Kevin},
  title = {Bayesian Inference with Tears: A Tutorial Workbook for Natural Language
	Researchers},
  url = {http://www.isi.edu/natural-language/people/bayes-with-tears.pdf},
}

@ARTICLE{GB11,
  author = {Gershman, Samuel J. and Blei, David M.},
  title = {A Tutorial on Bayesian Nonparametric Models},
  journal = {Journal of Mathematical Psychology},
  year = {2011},
  abstract = {A key problem in statistical modeling is model selection, that is,
	how to choose a model at an appropriate level of complexity. This
	problem appears in many settings, most prominently in choosing the
	number of clusters in mixture models or the number of factors in
	factor analysis. In this tutorial, we describe Bayesian nonparametric
	methods, a class of methods that side-steps this issue by allowing
	the data to determine the complexity of the model. This tutorial
	is a high-level introduction to Bayesian nonparametric methods and
	contains several examples of their application.},
  keywords = {Bayesian Methods; Chinese Restaurant Process; Indian Buffer Process},
}

@article{blei2003lda,
  author={David M. Blei and Andrew Ng and Michael Jordan},
  title={Latent Dirichlet allocation},
  journal={JMLR},
  year={2003},
  volume={3},
  pages={993-1022},
}

@article{teh2006hdp,
  author={Yee Whye Teh and Michael I. Jordan and Matthew J. Beal and David M. Blei},
  title={Hierarchical Dirichlet Processes},
  journal={JASA},
  year={2006},
  url={http://dx.doi.org/10.1198/016214506000000302},
  volume={101},
}

@inproceedings{hofmann1999plsa,
  author={Thomas Hofmann},
  title={Probilistic latent semantic analysis},
  booktitle={UAI},
  year={1999},
}

@inproceedings{Porteous2008,
 author = {Porteous, Ian and Newman, David and Ihler, Alexander and Asuncion, Arthur and Smyth, Padhraic and Welling, Max},
 title = {Fast collapsed gibbs sampling for latent dirichlet allocation},
 booktitle = {Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
 series = {KDD '08},
 year = {2008},
 isbn = {978-1-60558-193-4},
 location = {Las Vegas, Nevada, USA},
 pages = {569--577},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1401890.1401960},
 doi = {10.1145/1401890.1401960},
 acmid = {1401960},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {latent dirichlet allocation, sampling},
} 

@article{griffiths04finding,
  author={Thomas L. Griffiths and Mark Steyvers},
  title={Finding Scientific Topics},
  journal={PNAS},
  year={2004},
  volume={101},
  number={suppl. 1},
  pages={5228-5235},
}

@article{blei2011introduction,
  author={David M. Blei},
  title={Introduction to Probabilistic Topic Models},
  journal={Communications of the ACM},
  year={2011},
  url={http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf},
}

@inproceedings{rosenzvi2004author,
  author={Michal Rosen-Zvi and Tom Griffiths and Mark Steyvers and Padhraic Smyth},
  title={The Author-Topic Model for Authors and Documents},
  booktitle={UAI},
  year={2004},
}

@article{kullbackleibler51kl,
    author = {Kullback, S. and Leibler, R. A.},
    citeulike-article-id = {1616444},
    journal = {Annals of Mathematical Statistics},
    keywords = {bibtex-import},
    pages = {49--86},
    posted-at = {2007-09-03 16:53:17},
    priority = {0},
    title = {{On information and sufficiency}},
    volume = {22},
    year = {1951}
}